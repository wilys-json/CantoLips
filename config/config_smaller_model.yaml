
# Configuration for smaller model (if memory is still an issue)

data_root: "./Chinese-LiPS"
train_dir: "train"
val_dir: "val"
test_dir: "test"

hidden_dim: 128  # Smaller hidden dimension
temporal_model: "lstm"
num_lstm_layers: 1  # Fewer layers
num_transformer_layers: 4
num_heads: 4
dropout: 0.3

max_frames: 50  # Fewer frames
frame_height: 88  # Slightly smaller resolution
frame_width: 88
fps: 25

batch_size: 8
num_workers: 4
learning_rate: 0.0002
weight_decay: 0.00001
num_epochs: 150  # More epochs for smaller model
gradient_clip: 5.0
mixed_precision: true

warmup_epochs: 5
scheduler_type: "cosine"
step_size: 10
gamma: 0.5

checkpoint_dir: "./checkpoints_small"
save_every: 5
keep_last_n: 3

log_dir: "./logs_small"
log_every: 50

use_char_level: true
max_text_length: 200

device: "cuda"